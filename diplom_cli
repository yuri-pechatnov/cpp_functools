#!/usr/bin/env python3.6

'''

Examples:
    ./diplom_cli compile_bench  -o report.txt
    ./diplom_cli report_bench -i report.txt --filter_by exit_code=0 OptimizeLevel=2 --aggregate_by Realisation

    ./diplom_cli compile_bench --bench_repeat 3 -o compile_report.txt
    ./diplom_cli report_bench -i compile_report.txt --aggregate_by Realisation,Bench --heatmap user_time:Realisation*Bench  -o compile_report_o2_o3.png

    ./diplom_cli bench --meta_iterations=1000  --bench_repeat 20 -o report.txt
    ./diplom_cli report_bench -i report.txt --aggregate_by Realisation,Bench --heatmap UserProcessorTime:Realisation*Bench  -o report.png

'''

import argparse
import json
import logging
import os
import re
import subprocess
import sys
import pandas as pd
import types

from collections import defaultdict
from contextlib import contextmanager


def increase_prior_cmd():
    if os.stat("/usr/bin/chrt").st_mode & 0o4000:
        return "chrt -f 99 "
    else:
        logging.error("Don't use 'chrt'. Try set S_ISUID bit to its executable")
        return ""


def get_perf_stat_result(cmd, iters=1):
    p = subprocess.Popen(["bash", "-c", increase_prior_cmd() + "perf stat -x, -r {} -d {}".format(iters, cmd)],
                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = p.communicate()
    # print("STDOUT", stdout)
    # print("STDERR", stderr.decode('ascii'))
    ans = {}
    for line in stderr.decode('ascii').split('\n'):
        try:
            val, _, name, _ = line.split(',', 3)
            ans[name] = float(val)
        except ValueError:
            pass
    return stdout.decode('ascii'), ans



def root_path():
    return os.path.dirname(os.path.realpath(__file__))

@contextmanager
def work_in_root_path():
    old_pwd = os.getcwd()
    try:
        os.chdir(root_path())
        yield "Working in root path"
    finally:
        os.chdir(old_pwd)


def real_path(path_from_root):
    return os.path.join(root_path(), path_from_root)

def safe_shell_run(cmd):
    logging.info("exec: %s", cmd)
    assert(0 == os.system(cmd))

def run_and_get_output(cmd, **kwargs):
    logging.info("exec: %s", cmd)
    return subprocess.check_output(cmd, shell=True, **kwargs)

def print_s(s, fname=None):
    if fname:
        with open(fname, 'w') as f:
            f.write(s)
    else:
        print(s)

def all_functools_realisations():
    return os.listdir(real_path("functools/realisations"))

def check_functools_realisations(functools_realisations):
    for fr in functools_realisations:
        assert fr in all_functools_realisations(), f"Unknown realisation: {fr}"

def build_path():
    return real_path("build")

def tmp_build_path():
    return real_path("tmp_build")

def gtest_include_path():
    return real_path("googletest/googletest/include")

def gtest_static_lib_path():
    return real_path("build/libgtest.a")

def jsoncpp_include_path():
    return real_path("jsoncpp/include")

def jsoncpp_static_lib_path():
    return real_path("build/jsoncpp/src/lib_json/libjsoncpp.a")

def boost_range_include_path():
    return real_path("boost")

def range_v3_include_path():
    return real_path("range-v3/include")

def think_cell_include_path():
    return real_path("think-cell/range")

def gdb_bench_static_lib_path():
    return real_path("build/gdb_bench.a")

def gdb_bench_include_path():
    return real_path("functools/gdb_bench_lib")

def default_compilers():
    return ["g++", "clang++"]

def gcc_cmd(includes, compiler_bin):
    return f"{compiler_bin} -std=c++17 -isystem -pthread " + " ".join("-I" + i for i in includes)

def clear():
    with work_in_root_path():
        safe_shell_run("rm -rf build googletest jsoncpp")

def init():
    with work_in_root_path():
        if not os.path.exists(build_path()):
            os.mkdir(build_path())
        if not os.path.exists(tmp_build_path()):
            os.mkdir(tmp_build_path())

    with work_in_root_path():
        gtest_path = real_path("googletest")
        if not os.path.exists(gtest_path):
            safe_shell_run("git clone https://github.com/google/googletest.git")
            safe_shell_run(gcc_cmd(compiler_bin="g++", includes=[gtest_include_path(), real_path("googletest/googletest")]) +
                           " -c " + real_path("googletest/googletest/src/gtest-all.cc") +
                           " -o " + real_path("build/gtest-all.o"))
            safe_shell_run(f"ar -rv {gtest_static_lib_path()} build/gtest-all.o")

    with work_in_root_path():
        jsoncpp_path = os.path.join(root_path(), "jsoncpp")
        if not os.path.exists(jsoncpp_path):
            assert(0 == os.system("git clone https://github.com/open-source-parsers/jsoncpp.git"))
            build_dir = os.path.join(build_path(), "jsoncpp")
            os.mkdir(build_dir)
            os.chdir(build_dir)
            safe_shell_run(
                'cmake ' +
                '-DCMAKE_BUILD_TYPE=debug -DBUILD_STATIC_LIBS=ON -DBUILD_SHARED_LIBS=OFF ' +
                '-DARCHIVE_INSTALL_DIR=. -G "Unix Makefiles" ' + jsoncpp_path
            )
            safe_shell_run('make')

    with work_in_root_path():
        boost_path = real_path("boost")
        if not os.path.exists(boost_path):
            safe_shell_run("git clone --recursive https://github.com/boostorg/boost.git")
            os.chdir("boost")
            safe_shell_run("./bootstrap.sh --prefix=../build/boost/ --exec-prefix=../build/boost-exec/ --with-libraries=all")
            safe_shell_run("./b2 --prefix=../build/boost/ --exec-prefix=../build/boost-exec/ --build-dir=../build/boost-build")

    with work_in_root_path():
        range_v3_path = real_path("range-v3")
        if not os.path.exists(range_v3_path):
            safe_shell_run("git clone  https://github.com/ericniebler/range-v3.git")

    with work_in_root_path():
        think_cell_path = real_path("think-cell")
        if not os.path.exists(think_cell_path):
            os.mkdir(think_cell_path)
            os.chdir(think_cell_path)
            safe_shell_run("git clone https://github.com/think-cell/range.git")
            os.chdir(os.path.join(think_cell_path, "range"))
            safe_shell_run("git reset --hard 8851a489f9321e6a82b38680d5a2c5222582614c")

    with work_in_root_path():
        if not os.path.exists(gdb_bench_static_lib_path()):
            obj_file = gdb_bench_static_lib_path() + ".o"
            obj_file = gdb_bench_static_lib_path() + ".o"
            safe_shell_run(gcc_cmd(compiler_bin="g++", includes=[]) +
                           " -c " + os.path.join(gdb_bench_include_path(), "gdb_bench.cpp") +
                           " -o " + obj_file)
            safe_shell_run(f"ar -rv {gdb_bench_static_lib_path()} {obj_file}")


def run_test(functools_realisations, compilers):
    logging.info("run_test: %s", locals())
    init()
    check_functools_realisations(functools_realisations)
    for compiler in compilers:
        for realisation in functools_realisations:
            realisation_include = real_path(f"functools/realisations/{realisation}")
            test_source = real_path("functools/test/functools_test.cpp")
            test_binary = real_path(f"build/test_{realisation}")
            includes = [
                gtest_include_path(),
                boost_range_include_path(),
                range_v3_include_path(),
                think_cell_include_path(),
                real_path("functools/util"),
                realisation_include,
            ]
            safe_shell_run(gcc_cmd(includes=includes, compiler_bin=compiler) +
                           f" {test_source} " +
                           f" {gtest_static_lib_path()} " +
                           " -isystem -pthread -lpthread "
                           f" -D{realisation}_REALISATION " +
                           " -o " + test_binary)
            safe_shell_run(test_binary)


def run_bench(functools_realisations, compilers, cpu_set_command, meta_iterations, bench_repeat, output_file):
    logging.info("run_bench: %s", locals())
    init()

    check_functools_realisations(functools_realisations)
    bench_result = []

    def bench_exe_name(realisation, optimize_level, compiler):
        return real_path(f"tmp_build/bench_{realisation}_{compiler}_o{optimize_level}")

    def prepare_one_bench(realisation, optimize_level, compiler):
        realisation_include = real_path(f"functools/realisations/{realisation}")
        bench_source = real_path("functools/bench/bench.cpp")
        bench_binary = bench_exe_name(realisation, optimize_level, compiler)
        safe_shell_run(gcc_cmd(includes=[gtest_include_path(), boost_range_include_path(), range_v3_include_path(), think_cell_include_path(),
                                         jsoncpp_include_path(), gdb_bench_include_path(), real_path("functools/util"), realisation_include],
                               compiler_bin=compiler) +
                       f" -O{optimize_level} " +
                       f" -D{realisation}_REALISATION " +
                       bench_source +
                       f" {gtest_static_lib_path()} {jsoncpp_static_lib_path()} " +
                       " -o " + bench_binary)

    def run_one_bench(realisation, optimize_level, compiler):
        bench_binary = bench_exe_name(realisation, optimize_level, compiler)

        args = json.dumps({"MetaIterations": meta_iterations})

        batch_result = json.loads(run_and_get_output(f"{cpu_set_command} {bench_binary} '{args}'"))
        for bench, one_result in batch_result["Benchmarks"].items():
            one_result.update({
                "Bench": bench,
                "Realisation": realisation,
                "OptimizeLevel": optimize_level,
                "Compiler": compiler,
            })
            bench_result.append(one_result)

    params = [(compiler, optimize_level, realisation)
              for compiler in compilers
              for optimize_level in [2, 3]
              for realisation in functools_realisations]

    for compiler, optimize_level, realisation in params:
        prepare_one_bench(realisation, optimize_level, compiler)

    for i in range(bench_repeat):
        for compiler, optimize_level, realisation in params:
            run_one_bench(realisation, optimize_level, compiler)

    print_s(json.dumps(bench_result, indent=4), output_file)

    result_hashes = defaultdict(list)
    for br in bench_result:
        result_hashes[br["Bench"]].append(br["ResultHash"])
    for b_name, hashes in result_hashes.items():
        assert len(set(hashes)) == 1, f"Results are different for benchmark={b_name}"


def run_bench_with_gdb(functools_realisations, compilers, output_file):
    meta_iterations = 1
    bench_repeat = 1
    logging.info("run_bench_with_gdb: %s", locals())
    init()

    check_functools_realisations(functools_realisations)
    bench_result = []

    def bench_exe_name(realisation, optimize_level, compiler):
        return real_path(f"tmp_build/bench_with_gdb_{realisation}_{compiler}_o{optimize_level}")

    def prepare_one_bench(realisation, optimize_level, compiler):
        realisation_include = real_path(f"functools/realisations/{realisation}")
        bench_source = real_path("functools/bench/bench.cpp")
        bench_binary = bench_exe_name(realisation, optimize_level, compiler)
        safe_shell_run(gcc_cmd(includes=[gtest_include_path(), boost_range_include_path(), range_v3_include_path(), think_cell_include_path(),
                                         jsoncpp_include_path(), gdb_bench_include_path(), real_path("functools/util"), realisation_include],
                               compiler_bin=compiler) +
                       f" -g" +
                       f" -O{optimize_level} " +
                       f" -D{realisation}_REALISATION " +
                       bench_source +
                       f" {gtest_static_lib_path()} {jsoncpp_static_lib_path()} {gdb_bench_static_lib_path()}" +
                       " -o " + bench_binary)

    gdb_script = real_path('tmp_build/bench_gdb_script')
    gdb_log = real_path('tmp_build/bench_gdb_log')
    '''
        set logging file {gdb_log}
        set logging redirect on
        set logging overwrite on
        set logging off
        set logging on
    '''
    print_s('''
        set step-mode on
        set range-stepping off

        set confirm off
        set pagination off

        b BenchWithGdbStartLabel

        set $epoch = 0

        while true
            if ($epoch == 0)
                run
            else
                continue
            end
            finish

            set $count = 0
            while ($pc != BenchWithGdbFinishLabel)
                stepi
                set $count = $count + 1
            end

            printf "BENCH_WITH_GDB {\\"Instructions\\": %d, \\"RunId\\": %d}\\n", $count, $epoch

            set $epoch = $epoch + 1
        end
        quit
    ''', fname=gdb_script)


    def run_one_bench(realisation, optimize_level, compiler):
        bench_binary = bench_exe_name(realisation, optimize_level, compiler)
        bench_out = bench_binary + ".out"


        def get_gdb_result(basic_iterations):
            args = json.dumps({"MetaIterations": meta_iterations, "OutputFile": bench_out, "BasicIterations": basic_iterations})
            bench_keyword = "BENCH_WITH_GDB"
            cmd = f"gdb --batch --quiet --silent --command {gdb_script} --args {bench_binary} '{args}' | grep {bench_keyword}"
            gdb_result = run_and_get_output(cmd).decode('ascii')
            # logging.info("bench_with_gdb result is '%s'", gdb_result)
            gdb_result = [json.loads(line[len(bench_keyword) + 1:].strip()) for line in gdb_result.split("\n") if len(line) > len(bench_keyword)]
            gdb_result = {r["RunId"]: r for r in gdb_result}
            # logging.info("bench_with_gdb result is '%s'", gdb_result)

            with open(bench_out) as f:
                batch_result = json.load(f)

            return gdb_result, batch_result

        small_basic_iterations = 50
        delta_basic_iterations = 100
        gdb_result_small, batch_result = get_gdb_result(small_basic_iterations)
        gdb_result_big, batch_result = get_gdb_result(small_basic_iterations + delta_basic_iterations)

        for bench, one_result in batch_result["Benchmarks"].items():
            run_id = one_result["BenchWithGdbRunId"]
            one_result = {
                "Bench": bench,
                "Realisation": realisation,
                "OptimizeLevel": optimize_level,
                "Compiler": compiler,
                "Instructions": (gdb_result_big[run_id]["Instructions"] - gdb_result_small[run_id]["Instructions"]) / delta_basic_iterations,
                "ResultHash": one_result["ResultHash"],
            }
            bench_result.append(one_result)

    params = [(compiler, optimize_level, realisation)
              for compiler in compilers
              for optimize_level in [2, 3]
              for realisation in functools_realisations]

    for compiler, optimize_level, realisation in params:
        prepare_one_bench(realisation, optimize_level, compiler)

    for i in range(bench_repeat):
        for compiler, optimize_level, realisation in params:
            run_one_bench(realisation, optimize_level, compiler)

    print_s(json.dumps(bench_result, indent=4), output_file)

    result_hashes = defaultdict(list)
    for br in bench_result:
        result_hashes[br["Bench"]].append(br["ResultHash"])
    for b_name, hashes in result_hashes.items():
        assert len(set(hashes)) == 1, f"Results are different for benchmark={b_name}"


def run_compile_bench(functools_realisations, benchmarks, compilers, cpu_set_command, bench_repeat, output_file):
    logging.info("run_compile_bench: %s", locals())
    init()

    check_functools_realisations(functools_realisations)

    bench_source = real_path("functools/compile_bench/compile_bench.cpp")
    if len(benchmarks) == 0:
        with open(bench_source) as f:
            benchmarks = re.findall(r"\W(\w*)_BENCH", f.read())

    logging.info("Realisations: %s, benchmarks: %s", functools_realisations, benchmarks)

    def test_time_cmd(time_cmd):
        try:
            subprocess.check_output(f"{time_cmd} -f '' echo 1  2>&1 > /dev/null", shell=True, stderr=subprocess.STDOUT)
            return True
        except:
            return False


    time_command = "time"
    if not test_time_cmd(time_command):
        time_command = "gtime" # on MacOS: brew install gnu-time
    assert test_time_cmd(time_command), "No proper command time or gtime in shell"
    logging.info(f"Use time-command: {time_command}")
    time_format = '{"real_time":%e, "exit_code":%x, "user_time":%U,"system_time":%S}'

    bench_result = []

    def run_one_bench(realisation, bench, optimize_level, compiler):
        realisation_include = real_path(f"functools/realisations/{realisation}")
        params_key = f"{realisation}_{bench}_{compiler}_{optimize_level}"
        bench_result_report = real_path(f"tmp_build/compile_bench_{params_key}.report")
        bench_result_lib = real_path(f"tmp_build/compile_bench_{params_key}.o")
        bench_compiler_message = real_path(f"tmp_build/compile_bench_{params_key}.compiler_message")
        try:
            _ = run_and_get_output(
                f"{cpu_set_command} " +
                f"{time_command} --quiet -f '{time_format}' -o {bench_result_report} " +
                gcc_cmd(includes=[gtest_include_path(), boost_range_include_path(), range_v3_include_path(), think_cell_include_path(),
                                  jsoncpp_include_path(), real_path("functools/util"), realisation_include],
                        compiler_bin=compiler) +
                f" -O{optimize_level} " +
                f" -D{realisation}_REALISATION " +
                f" -D{bench}_BENCH " +
                bench_source +
                f" {gtest_static_lib_path()} {jsoncpp_static_lib_path()} " +
                " -c " +
                " -o " + bench_result_lib +
                f" 2>&1 1>{bench_compiler_message}",
                stderr=subprocess.STDOUT,
            )
            with open(bench_result_report) as f:
                one_result = f.read()
            one_result = json.loads(one_result)
        except subprocess.CalledProcessError as e:
            one_result = {
                "exit_code": e.returncode
            }
            logging.warn(f"Compilation failed. Params = {params_key}")
        one_result.update({
            "Realisation": realisation,
            "Bench": bench,
            "OptimizeLevel": optimize_level,
            "BinarySize": os.stat(bench_result_lib).st_size if one_result["exit_code"] == 0 else None,
            "Compiler": compiler,
        })

        print(json.dumps(one_result))
        bench_result.append(one_result)

    for i in range(bench_repeat):
        for compiler in compilers:
            for optimize_level in [2, 3]:
                for bench in benchmarks:
                    for realisation in functools_realisations:
                        run_one_bench(realisation, bench, optimize_level, compiler)

    print_s(json.dumps(bench_result, indent=4, sort_keys=True), fname=output_file)


def report_bench(input_file, output_file, aggregate_by, filter_by, heatmap):
    logging.info("report_bench: %s", locals())
    with open(input_file) as f:
        report = pd.DataFrame(json.load(f))
    for filter_pred in filter_by:
        try:
            def predicate(df):
                return eval(filter_pred)
            #~ field, value = filter_pair.strip().split('=')
            #print(report)
            #~ field_type = type(report.loc[report.index[0], field])
            #~ logging.info("Filter %s == %s(%s)", field, repr(field_type), value)
            report = report[predicate(report)]
        except Exception as e:
            logging.exception("Can't apply filter %s=%s", field, value)
    for aggr in aggregate_by:
        report = report.groupby(aggr.split(',')).mean()
    if heatmap:
        target, X, Y = re.findall(r"(.*):(.*)\*(.*)", heatmap)[0]
        hmap = defaultdict(dict)
        for (x, y), idxs in report.groupby([X, Y]).groups.items():
            hmap[x][y] = report.loc[idxs, target].mean()
        report = pd.DataFrame(hmap)

    if output_file.endswith(".html"):
        print_s(report.to_html(), output_file)
    elif output_file.endswith(".png") or output_file.endswith(".pdf"):
        assert heatmap and output_file

        import seaborn
        import matplotlib.colors as colors

        seaborn.set(rc={'figure.figsize':(15,10)})
        seaborn.heatmap(report, annot=True, cmap="YlOrBr").get_figure().savefig(output_file)
    else:
        print(report)


if __name__ == "__main__":
    logging.basicConfig(format=u'%(filename)s[LINE:%(lineno)d]# %(levelname)-8s [%(asctime)s]  %(message)s',
                        level=logging.INFO)

    parser = argparse.ArgumentParser(
        "Iurii Pechatnov's diploma cli",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        #~ formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        epilog=__doc__
    )
    subparsers = parser.add_subparsers()

    class ArgumentParserWithDefaults(argparse.ArgumentParser):
        def add_argument(self, *args, help=None, default=None, **kwargs):
            if help is not None:
                kwargs['help'] = help
            if default is not None and args[0] != '-h':
                kwargs['default'] = default
                if help is not None:
                    kwargs['help'] += ' Default: {}'.format(default)
            super().add_argument(*args, **kwargs)

    def add_arg(self, *args, **kwargs):
        if 'default' in kwargs and 'help' not in kwargs:
            kwargs['help'] = f"Default: {kwargs['default']}"
        self.add_argument(*args, **kwargs)

    @contextmanager
    def subparser(name, f):
        p = subparsers.add_parser(name)
        p.add_arg = types.MethodType(add_arg, p)
        yield p

        def func(args):
            d = args.__dict__.copy()
            del d['func']
            f(**d)

        p.set_defaults(func=func)

    with subparser('init', init) as p:
        pass
    with subparser('clear', clear) as p:
        pass
    with subparser('test', run_test) as p:
        p.add_arg('-r', '--functools_realisations', nargs='*', default=all_functools_realisations(), type=str)
        p.add_arg('--compilers', nargs='*', default=default_compilers(), type=str)
    with subparser('bench', run_bench) as p:
        p.add_arg('-r', '--functools_realisations', nargs='*', default=all_functools_realisations(), type=str)
        p.add_arg('-m', '--meta_iterations', default=3, type=int)
        p.add_arg('--compilers', nargs='*', default=default_compilers(), type=str)
        p.add_arg('--cpu_set_command', default="", type=str)
        p.add_arg('--bench_repeat', default=1, type=int)
        p.add_arg('-o', '--output_file', default='', type=str)
    with subparser('bench_with_gdb', run_bench_with_gdb) as p:
        p.add_arg('-r', '--functools_realisations', nargs='*', default=all_functools_realisations(), type=str)
        p.add_arg('--compilers', nargs='*', default=default_compilers(), type=str)
        p.add_arg('-o', '--output_file', default='', type=str)
    with subparser('compile_bench', run_compile_bench) as p:
        p.add_arg('-r', '--functools_realisations', nargs='*', default=all_functools_realisations(), type=str)
        p.add_arg('-b', '--benchmarks', nargs='*', default=[], type=str)
        p.add_arg('--compilers', nargs='*', default=default_compilers(), type=str)
        p.add_arg('--cpu_set_command', default="", type=str)
        p.add_arg('--bench_repeat', default=1, type=int)
        p.add_arg('-o', '--output_file', default='', type=str)
    with subparser('report_bench', report_bench) as p:
        p.add_arg('-i', '--input_file', default='', type=str)
        p.add_arg('--aggregate_by', nargs='*', default=[], type=str)
        p.add_arg('--filter_by', nargs='*', default=[], type=str, help="Example: \"df.Realisation == 'baseline'\"")
        p.add_arg('--heatmap', default='', type=str)
        p.add_arg('-o', '--output_file', default='', type=str)

    args = parser.parse_args()
    args.func(args)

