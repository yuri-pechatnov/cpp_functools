#!/usr/bin/env python3

'''

Examples:
    ./diplom_cli compile_bench  -o report.txt
    ./diplom_cli report_bench -i report.txt --filter_by exit_code=0 OptimizeLevel=2 --aggregate_by Realisation

    ./diplom_cli compile_bench --bench_repeat 3 -o compile_report.txt
    ./diplom_cli report_bench -i compile_report.txt --aggregate_by Realisation,Bench --heatmap user_time:Realisation*Bench  -o compile_report_o2_o3.png

    ./diplom_cli bench --meta_iterations=1000  --bench_repeat 20 -o report.txt
    ./diplom_cli report_bench -i report.txt --aggregate_by Realisation,Bench --heatmap UserProcessorTime:Realisation*Bench  -o report.png

'''

import argparse
import json
import logging
import os
import re
import subprocess
import sys
import pandas as pd
import types

from collections import defaultdict
from contextlib import contextmanager


def root_path():
    return os.path.dirname(os.path.realpath(__file__))

@contextmanager
def work_in_root_path():
    old_pwd = os.getcwd()
    try:
        os.chdir(root_path())
        yield "Working in root path"
    finally:
        os.chdir(old_pwd)


def real_path(path_from_root):
    return os.path.join(root_path(), path_from_root)

def safe_shell_run(cmd):
    logging.info("exec: %s", cmd)
    assert(0 == os.system(cmd))

def run_and_get_output(cmd, **kwargs):
    logging.info("exec: %s", cmd)
    return subprocess.check_output(cmd, shell=True, **kwargs)

def print_s(s, fname=None):
    if fname:
        with open(fname, 'w') as f:
            f.write(s)
    else:
        print(s)

def all_functools_realisations():
    return os.listdir(real_path("functools/realisations"))

def check_functools_realisations(functools_realisations):
    for fr in functools_realisations:
        assert fr in all_functools_realisations(), f"Unknown realisation: {fr}"

def build_path():
    return real_path("build")

def gtest_include_path():
    return real_path("googletest/googletest/include")

def gtest_static_lib_path():
    return real_path("build/libgtest.a")

def jsoncpp_include_path():
    return real_path("jsoncpp/include")

def jsoncpp_static_lib_path():
    return real_path("build/jsoncpp/src/lib_json/libjsoncpp.a")

def boost_range_include_path():
    return real_path("boost")

def range_v3_include_path():
    return real_path("range-v3/include")

def think_cell_include_path():
    return real_path("think-cell/range")

def default_compilers():
    return ["g++", "clang++"]

def gcc_cmd(includes, compiler_bin):
    return f"{compiler_bin} -std=c++17 -isystem -pthread " + " ".join("-I" + i for i in includes)

def clear():
    with work_in_root_path():
        safe_shell_run("rm -rf build googletest jsoncpp")

def init():
    with work_in_root_path():
        if not os.path.exists(build_path()):
            os.mkdir(build_path())

    with work_in_root_path():
        gtest_path = real_path("googletest")
        if not os.path.exists(gtest_path):
            safe_shell_run("git clone https://github.com/google/googletest.git")
            safe_shell_run(gcc_cmd(compiler_bin="g++", includes=[gtest_include_path(), real_path("googletest/googletest")]) +
                           " -c " + real_path("googletest/googletest/src/gtest-all.cc") +
                           " -o " + real_path("build/gtest-all.o"))
            safe_shell_run(f"ar -rv {gtest_static_lib_path()} build/gtest-all.o")

    with work_in_root_path():
        jsoncpp_path = os.path.join(root_path(), "jsoncpp")
        if not os.path.exists(jsoncpp_path):
            assert(0 == os.system("git clone https://github.com/open-source-parsers/jsoncpp.git"))
            build_dir = os.path.join(build_path(), "jsoncpp")
            os.mkdir(build_dir)
            os.chdir(build_dir)
            safe_shell_run(
                'cmake ' +
                '-DCMAKE_BUILD_TYPE=debug -DBUILD_STATIC_LIBS=ON -DBUILD_SHARED_LIBS=OFF ' +
                '-DARCHIVE_INSTALL_DIR=. -G "Unix Makefiles" ' + jsoncpp_path
            )
            safe_shell_run('make')

    with work_in_root_path():
        boost_path = real_path("boost")
        if not os.path.exists(boost_path):
            safe_shell_run("git clone --recursive https://github.com/boostorg/boost.git")
            os.chdir("boost")
            safe_shell_run("./bootstrap.sh --prefix=../build/boost/ --exec-prefix=../build/boost-exec/ --with-libraries=all")
            safe_shell_run("./b2 --prefix=../build/boost/ --exec-prefix=../build/boost-exec/ --build-dir=../build/boost-build")

    with work_in_root_path():
        range_v3_path = real_path("range-v3")
        if not os.path.exists(range_v3_path):
            safe_shell_run("git clone  https://github.com/ericniebler/range-v3.git")

    with work_in_root_path():
        think_cell_path = real_path("think-cell")
        if not os.path.exists(think_cell_path):
            os.mkdir(think_cell_path)
            os.chdir(think_cell_path)
            safe_shell_run("git clone https://github.com/think-cell/range.git")


def run_test(functools_realisations, compilers):
    logging.info("run_test: %s", locals())
    init()
    check_functools_realisations(functools_realisations)
    for compiler in compilers:
        for realisation in functools_realisations:
            realisation_include = real_path(f"functools/realisations/{realisation}")
            test_source = real_path("functools/test/functools_test.cpp")
            test_binary = real_path(f"build/test_{realisation}")
            includes = [
                gtest_include_path(),
                boost_range_include_path(),
                range_v3_include_path(),
                think_cell_include_path(),
                real_path("functools/util"),
                realisation_include,
            ]
            safe_shell_run(gcc_cmd(includes=includes, compiler_bin=compiler) +
                           f" {gtest_static_lib_path()} " +
                           f" -D{realisation}_REALISATION " +
                           test_source +
                           " -o " + test_binary)
            safe_shell_run(test_binary)


def run_bench(functools_realisations, compilers, meta_iterations, bench_repeat, output_file):
    logging.info("run_bench: %s", locals())
    init()

    check_functools_realisations(functools_realisations)
    bench_result = []

    def bench_exe_name(realisation, optimize_level, compiler):
        return real_path(f"build/bench_{realisation}_{compiler}_o{optimize_level}")

    def prepare_one_bench(realisation, optimize_level, compiler):
        realisation_include = real_path(f"functools/realisations/{realisation}")
        bench_source = real_path("functools/bench/bench.cpp")
        bench_binary = bench_exe_name(realisation, optimize_level, compiler)
        safe_shell_run(gcc_cmd(includes=[gtest_include_path(), boost_range_include_path(), range_v3_include_path(), think_cell_include_path(),
                                         jsoncpp_include_path(), real_path("functools/util"), realisation_include],
                               compiler_bin=compiler) +
                       f" -O{optimize_level} " +
                       f" -D{realisation}_REALISATION " +
                       f" {gtest_static_lib_path()} {jsoncpp_static_lib_path()} " +
                       bench_source +
                       " -o " + bench_binary)

    def run_one_bench(realisation, optimize_level, compiler):
        bench_binary = bench_exe_name(realisation, optimize_level, compiler)

        args = json.dumps({"MetaIterations": meta_iterations})

        batch_result = json.loads(run_and_get_output(f"{bench_binary} '{args}'"))
        for bench, one_result in batch_result["Benchmarks"].items():
            one_result.update({
                "Bench": bench,
                "Realisation": realisation,
                "OptimizeLevel": optimize_level,
                "Compiler": compiler,
            })
            bench_result.append(one_result)

    params = [(compiler, optimize_level, realisation)
              for compiler in compilers
              for optimize_level in [2, 3]
              for realisation in functools_realisations]

    for compiler, optimize_level, realisation in params:
        prepare_one_bench(realisation, optimize_level, compiler)

    for i in range(bench_repeat):
        for compiler, optimize_level, realisation in params:
            run_one_bench(realisation, optimize_level, compiler)

    print_s(json.dumps(bench_result, indent=4), output_file)

    result_hashes = defaultdict(list)
    for br in bench_result:
        result_hashes[br["Bench"]].append(br["ResultHash"])
    for b_name, hashes in result_hashes.items():
        assert len(set(hashes)) == 1, f"Results are different for benchmark={b_name}"


def run_compile_bench(functools_realisations, benchmarks, compilers, bench_repeat, output_file):
    logging.info("run_compile_bench: %s", locals())
    init()

    check_functools_realisations(functools_realisations)

    bench_source = real_path("functools/compile_bench/compile_bench.cpp")
    if len(benchmarks) == 0:
        with open(bench_source) as f:
            benchmarks = re.findall(r"\W(\w*)_BENCH", f.read())

    logging.info("Realisations: %s, benchmarks: %s", functools_realisations, benchmarks)

    def test_time_cmd(time_cmd):
        try:
            subprocess.check_output(f"{time_cmd} -f '' echo 1  2>&1 > /dev/null", shell=True, stderr=subprocess.STDOUT)
            return True
        except:
            return False


    time_command = "time"
    if not test_time_cmd(time_command):
        time_command = "gtime" # on MacOS: brew install gnu-time
    assert test_time_cmd(time_command), "No proper command time or gtime in shell"
    logging.info(f"Use time-command: {time_command}")
    time_format = '{"real_time":%e, "exit_code":%x, "user_time":%U,"system_time":%S}'

    bench_result = []

    def run_one_bench(realisation, bench, optimize_level, compiler):
        realisation_include = real_path(f"functools/realisations/{realisation}")
        params_key = f"{realisation}_{bench}_{compiler}_{optimize_level}"
        bench_result_report = real_path(f"build/compile_bench_{params_key}.report")
        bench_result_lib = real_path(f"build/compile_bench_{params_key}.o")
        bench_compiler_message = real_path(f"build/compile_bench_{params_key}.compiler_message")
        try:
            _ = run_and_get_output(
                f"{time_command} --quiet -f '{time_format}' -o {bench_result_report} " +
                gcc_cmd(includes=[gtest_include_path(), boost_range_include_path(), range_v3_include_path(), think_cell_include_path(),
                                  jsoncpp_include_path(), real_path("functools/util"), realisation_include],
                        compiler_bin=compiler) +
                f" -O{optimize_level} " +
                f" -D{realisation}_REALISATION " +
                f" -D{bench}_BENCH " +
                f" {gtest_static_lib_path()} {jsoncpp_static_lib_path()} " +
                " -c " +
                bench_source +
                " -o " + bench_result_lib +
                f" 2>&1 1>{bench_compiler_message}",
                stderr=subprocess.STDOUT,
            )
            with open(bench_result_report) as f:
                one_result = f.read()
            one_result = json.loads(one_result)
        except subprocess.CalledProcessError as e:
            one_result = {
                "exit_code": e.returncode
            }
            logging.warn(f"Compilation failed. Params = {params_key}")
        one_result.update({
            "Realisation": realisation,
            "Bench": bench,
            "OptimizeLevel": optimize_level,
            "BinarySize": os.stat(bench_result_lib).st_size if one_result["exit_code"] == 0 else None,
            "Compiler": compiler,
        })

        print(json.dumps(one_result))
        bench_result.append(one_result)

    for i in range(bench_repeat):
        for compiler in compilers:
            for optimize_level in [2, 3]:
                for bench in benchmarks:
                    for realisation in functools_realisations:
                        run_one_bench(realisation, bench, optimize_level, compiler)

    print_s(json.dumps(bench_result, indent=4, sort_keys=True), fname=output_file)


def report_bench(input_file, output_file, aggregate_by, filter_by, heatmap):
    logging.info("report_bench: %s", locals())
    with open(input_file) as f:
        report = pd.DataFrame(json.load(f))
    for filter_pair in filter_by:
        try:
            field, value = filter_pair.strip().split('=')
            #print(report)
            field_type = type(report.loc[report.index[0], field])
            logging.info("Filter %s == %s(%s)", field, repr(field_type), value)
            report = report[report[field] == field_type(value)]
        except Exception as e:
            logging.exception("Can't apply filter %s=%s", field, value)
    for aggr in aggregate_by:
        report = report.groupby(aggr.split(',')).mean()
    if heatmap:
        target, X, Y = re.findall(r"(.*):(.*)\*(.*)", heatmap)[0]
        hmap = defaultdict(dict)
        for (x, y), idxs in report.groupby([X, Y]).groups.items():
            hmap[x][y] = report.loc[idxs, target].mean()
        report = pd.DataFrame(hmap)

    if output_file.endswith(".html"):
        print_s(report.to_html(), output_file)
    elif output_file.endswith(".png") or output_file.endswith(".pdf"):
        assert heatmap and output_file

        import seaborn
        import matplotlib.colors as colors

        seaborn.set(rc={'figure.figsize':(15,10)})
        seaborn.heatmap(report, annot=True, cmap="YlOrBr", norm=colors.LogNorm(0, report.max())).get_figure().savefig(output_file)
    else:
        print(report)


if __name__ == "__main__":
    logging.basicConfig(format=u'%(filename)s[LINE:%(lineno)d]# %(levelname)-8s [%(asctime)s]  %(message)s',
                        level=logging.INFO)

    parser = argparse.ArgumentParser(
        "Iurii Pechatnov's diploma cli",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        #~ formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        epilog=__doc__
    )
    subparsers = parser.add_subparsers()

    class ArgumentParserWithDefaults(argparse.ArgumentParser):
        def add_argument(self, *args, help=None, default=None, **kwargs):
            if help is not None:
                kwargs['help'] = help
            if default is not None and args[0] != '-h':
                kwargs['default'] = default
                if help is not None:
                    kwargs['help'] += ' Default: {}'.format(default)
            super().add_argument(*args, **kwargs)

    def add_arg(self, *args, **kwargs):
        if 'default' in kwargs and 'help' not in kwargs:
            kwargs['help'] = f"Default: {kwargs['default']}"
        self.add_argument(*args, **kwargs)

    @contextmanager
    def subparser(name, f):
        p = subparsers.add_parser(name)
        p.add_arg = types.MethodType(add_arg, p)
        yield p

        def func(args):
            d = args.__dict__.copy()
            del d['func']
            f(**d)

        p.set_defaults(func=func)

    with subparser('init', init) as p:
        pass
    with subparser('clear', clear) as p:
        pass
    with subparser('test', run_test) as p:
        p.add_arg('-r', '--functools_realisations', nargs='*', default=all_functools_realisations(), type=str)
        p.add_arg('--compilers', nargs='*', default=default_compilers(), type=str)
    with subparser('bench', run_bench) as p:
        p.add_arg('-r', '--functools_realisations', nargs='*', default=all_functools_realisations(), type=str)
        p.add_arg('-m', '--meta_iterations', default=3, type=int)
        p.add_arg('--compilers', nargs='*', default=default_compilers(), type=str)
        p.add_arg('--bench_repeat', default=1, type=int)
        p.add_arg('-o', '--output_file', default='', type=str)
    with subparser('compile_bench', run_compile_bench) as p:
        p.add_arg('-r', '--functools_realisations', nargs='*', default=all_functools_realisations(), type=str)
        p.add_arg('-b', '--benchmarks', nargs='*', default=[], type=str)
        p.add_arg('--compilers', nargs='*', default=default_compilers(), type=str)
        p.add_arg('--bench_repeat', default=1, type=int)
        p.add_arg('-o', '--output_file', default='', type=str)
    with subparser('report_bench', report_bench) as p:
        p.add_arg('-i', '--input_file', default='', type=str)
        p.add_arg('--aggregate_by', nargs='*', default=[], type=str)
        p.add_arg('--filter_by', nargs='*', default=[], type=str)
        p.add_arg('--heatmap', default='', type=str)
        p.add_arg('-o', '--output_file', default='', type=str)

    args = parser.parse_args()
    args.func(args)

